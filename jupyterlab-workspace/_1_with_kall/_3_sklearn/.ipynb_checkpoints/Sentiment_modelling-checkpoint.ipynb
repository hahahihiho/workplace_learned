{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGABYTE\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 넘파이 배열로 저장된 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "INPUT_TRAIN_DATA = 'review_train_input.npy'\n",
    "LABEL_TRAIN_DATA = 'review_train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "input_data = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA, 'rb'))\n",
    "label_data = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA, 'rb'))\n",
    "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "VOCAB_SIZE = prepro_configs['vocab_size']\n",
    "EMB_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "input_train, input_eval, label_train, label_eval = train_test_split(input_data, label_data, test_size=TEST_SPLIT, random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43756"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 8, 32)             1400192   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,408,449\n",
      "Trainable params: 1,408,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "135000/135000 [==============================] - 54s 398us/step - loss: 0.4403 - acc: 0.7917 - val_loss: 0.4184 - val_acc: 0.8051\n",
      "Epoch 2/10\n",
      "135000/135000 [==============================] - 53s 394us/step - loss: 0.3974 - acc: 0.8193 - val_loss: 0.4137 - val_acc: 0.8073\n",
      "Epoch 3/10\n",
      "135000/135000 [==============================] - 54s 397us/step - loss: 0.3802 - acc: 0.8306 - val_loss: 0.4070 - val_acc: 0.8155\n",
      "Epoch 4/10\n",
      "135000/135000 [==============================] - 46s 339us/step - loss: 0.3638 - acc: 0.8404 - val_loss: 0.4099 - val_acc: 0.8133\n",
      "Epoch 5/10\n",
      "135000/135000 [==============================] - 45s 336us/step - loss: 0.3496 - acc: 0.8494 - val_loss: 0.4196 - val_acc: 0.8073\n",
      "Epoch 6/10\n",
      "135000/135000 [==============================] - 53s 394us/step - loss: 0.3373 - acc: 0.8566 - val_loss: 0.4216 - val_acc: 0.8087\n",
      "Epoch 7/10\n",
      "135000/135000 [==============================] - 53s 395us/step - loss: 0.3257 - acc: 0.8643 - val_loss: 0.4292 - val_acc: 0.8053\n",
      "Epoch 8/10\n",
      "135000/135000 [==============================] - 54s 397us/step - loss: 0.3145 - acc: 0.8701 - val_loss: 0.4441 - val_acc: 0.8065\n",
      "Epoch 9/10\n",
      "135000/135000 [==============================] - 54s 397us/step - loss: 0.3037 - acc: 0.8753 - val_loss: 0.4465 - val_acc: 0.7986\n",
      "Epoch 10/10\n",
      "135000/135000 [==============================] - 54s 399us/step - loss: 0.2936 - acc: 0.8809 - val_loss: 0.4547 - val_acc: 0.8012\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Embedding(input_dim = VOCAB_SIZE, output_dim=EMB_SIZE, input_length=8))\n",
    "model1.add(Flatten())\n",
    "\n",
    "# 학습층 추가\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "\n",
    "# 분류기를 추가\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model1.summary()\n",
    "\n",
    "history1 = model1.fit(input_train, label_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(input_eval, label_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEST_DATA = 'review_test_input.npy'\n",
    "LABEL_TEST_DATA = 'review_test_label.npy'\n",
    "\n",
    "input_test = np.load(open(DATA_IN_PATH + INPUT_TEST_DATA, 'rb'))\n",
    "input_label = np.load(open(DATA_IN_PATH + LABEL_TEST_DATA, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 4s 82us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model1.evaluate(input_test,input_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc : 0.80286\n"
     ]
    }
   ],
   "source": [
    "print('test_acc : {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    \n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_acc',\n",
    "        patience = 3,\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = 'review_lstm.h5',\n",
    "        monitor = 'val_loss',\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "135000/135000 [==============================] - 72s 536us/step - loss: 0.4363 - acc: 0.7931 - val_loss: 0.4124 - val_acc: 0.8076\n",
      "Epoch 2/10\n",
      "135000/135000 [==============================] - 70s 515us/step - loss: 0.3900 - acc: 0.8234 - val_loss: 0.4001 - val_acc: 0.8243\n",
      "Epoch 3/10\n",
      "135000/135000 [==============================] - 65s 485us/step - loss: 0.3636 - acc: 0.8397 - val_loss: 0.3827 - val_acc: 0.8266\n",
      "Epoch 4/10\n",
      "135000/135000 [==============================] - 66s 486us/step - loss: 0.3455 - acc: 0.8502 - val_loss: 0.3796 - val_acc: 0.8316\n",
      "Epoch 5/10\n",
      "135000/135000 [==============================] - 66s 485us/step - loss: 0.3303 - acc: 0.8592 - val_loss: 0.3848 - val_acc: 0.8292\n",
      "Epoch 6/10\n",
      "135000/135000 [==============================] - 65s 482us/step - loss: 0.3184 - acc: 0.8654 - val_loss: 0.3867 - val_acc: 0.8290\n",
      "Epoch 7/10\n",
      "135000/135000 [==============================] - 65s 482us/step - loss: 0.3096 - acc: 0.8694 - val_loss: 0.3832 - val_acc: 0.8291\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim = VOCAB_SIZE, output_dim=EMB_SIZE))\n",
    "lstm_model.add(CuDNNLSTM(32))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "lstm_history = lstm_model.fit(input_train, label_train,\n",
    "                              epochs=10,\n",
    "                              batch_size=32,\n",
    "                              validation_data=(input_eval, label_eval),\n",
    "                              callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list1 = [\n",
    "    \n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_acc',\n",
    "        patience = 5,\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = 'review_lstm1.h5',\n",
    "        monitor = 'val_loss',\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "135000/135000 [==============================] - 65s 483us/step - loss: 0.4353 - acc: 0.7940 - val_loss: 0.4088 - val_acc: 0.8154\n",
      "Epoch 2/10\n",
      "135000/135000 [==============================] - 64s 477us/step - loss: 0.3853 - acc: 0.8257 - val_loss: 0.3868 - val_acc: 0.8248\n",
      "Epoch 3/10\n",
      "135000/135000 [==============================] - 64s 471us/step - loss: 0.3583 - acc: 0.8427 - val_loss: 0.3814 - val_acc: 0.8279\n",
      "Epoch 4/10\n",
      "135000/135000 [==============================] - 65s 479us/step - loss: 0.3421 - acc: 0.8525 - val_loss: 0.3810 - val_acc: 0.8286\n",
      "Epoch 5/10\n",
      "135000/135000 [==============================] - 64s 476us/step - loss: 0.3301 - acc: 0.8592 - val_loss: 0.3798 - val_acc: 0.8305\n",
      "Epoch 6/10\n",
      "135000/135000 [==============================] - 64s 475us/step - loss: 0.3207 - acc: 0.8640 - val_loss: 0.3821 - val_acc: 0.8291\n",
      "Epoch 7/10\n",
      "135000/135000 [==============================] - 64s 473us/step - loss: 0.3128 - acc: 0.8682 - val_loss: 0.3833 - val_acc: 0.8316\n",
      "Epoch 8/10\n",
      "135000/135000 [==============================] - 64s 474us/step - loss: 0.3057 - acc: 0.8720 - val_loss: 0.3883 - val_acc: 0.8277\n",
      "Epoch 9/10\n",
      "135000/135000 [==============================] - 64s 476us/step - loss: 0.2987 - acc: 0.8748 - val_loss: 0.3859 - val_acc: 0.8294\n",
      "Epoch 10/10\n",
      "135000/135000 [==============================] - 64s 476us/step - loss: 0.2920 - acc: 0.8789 - val_loss: 0.3884 - val_acc: 0.8311\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "\n",
    "lstm_model1 = Sequential()\n",
    "lstm_model1.add(Embedding(input_dim = VOCAB_SIZE, output_dim=EMB_SIZE))\n",
    "lstm_model1.add(CuDNNLSTM(64))\n",
    "lstm_model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model1.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "lstm_history1 = lstm_model1.fit(input_train, label_train,\n",
    "                                epochs=10,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(input_eval, label_eval),\n",
    "                                callbacks=callbacks_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "135000/135000 [==============================] - 83s 615us/step - loss: 0.4360 - acc: 0.7929 - val_loss: 0.4171 - val_acc: 0.8062\n",
      "Epoch 2/10\n",
      "135000/135000 [==============================] - 81s 597us/step - loss: 0.3873 - acc: 0.8245 - val_loss: 0.3894 - val_acc: 0.8213\n",
      "Epoch 3/10\n",
      "135000/135000 [==============================] - 82s 605us/step - loss: 0.3611 - acc: 0.8411 - val_loss: 0.3820 - val_acc: 0.8315\n",
      "Epoch 4/10\n",
      "135000/135000 [==============================] - 83s 613us/step - loss: 0.3432 - acc: 0.8522 - val_loss: 0.3817 - val_acc: 0.8297\n",
      "Epoch 5/10\n",
      "135000/135000 [==============================] - 81s 597us/step - loss: 0.3295 - acc: 0.8593 - val_loss: 0.3849 - val_acc: 0.8297\n",
      "Epoch 6/10\n",
      "135000/135000 [==============================] - 83s 612us/step - loss: 0.3184 - acc: 0.8660 - val_loss: 0.3870 - val_acc: 0.8284\n",
      "Epoch 7/10\n",
      "135000/135000 [==============================] - 83s 615us/step - loss: 0.3099 - acc: 0.8700 - val_loss: 0.3899 - val_acc: 0.8263\n",
      "Epoch 8/10\n",
      "135000/135000 [==============================] - 82s 604us/step - loss: 0.3023 - acc: 0.8733 - val_loss: 0.3899 - val_acc: 0.8299\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "\n",
    "lstm_model2 = Sequential()\n",
    "lstm_model2.add(Embedding(input_dim = VOCAB_SIZE, output_dim=EMB_SIZE))\n",
    "lstm_model2.add(CuDNNLSTM(32, return_sequences = True))\n",
    "lstm_model2.add(CuDNNLSTM(16))\n",
    "lstm_model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model2.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "lstm_history2 = lstm_model2.fit(input_train, label_train,\n",
    "                                epochs=10,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(input_eval, label_eval),\n",
    "                                callbacks=callbacks_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < Note >\n",
    "    \n",
    "    훈련 정확도 자체가 많이 증가되지 않는 것은 데이터가 불충분해서 일 수 있다. 현재 데이터의 양(quantity)이 불충분하지는 않으므로,\n",
    "    질(quality)의 불충분이라 가정하고 embedding dimensionality 역시 조정해보자\n",
    "    \n",
    "    딕셔너리의 크기에 비해 임베딩 차원 수가 부족해 전체 단어를 잘 표현하고 있지 못할 가능성이 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding dimesionality조정\n",
    "\n",
    "- EMB_SIZE * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "135000/135000 [==============================] - 86s 639us/step - loss: 0.4313 - acc: 0.7953 - val_loss: 0.4072 - val_acc: 0.8094\n",
      "Epoch 2/10\n",
      "135000/135000 [==============================] - 85s 630us/step - loss: 0.3781 - acc: 0.8307 - val_loss: 0.3847 - val_acc: 0.8269\n",
      "Epoch 3/10\n",
      "135000/135000 [==============================] - 85s 630us/step - loss: 0.3500 - acc: 0.8479 - val_loss: 0.3760 - val_acc: 0.8318\n",
      "Epoch 4/10\n",
      "135000/135000 [==============================] - 84s 626us/step - loss: 0.3306 - acc: 0.8587 - val_loss: 0.3790 - val_acc: 0.8323\n",
      "Epoch 5/10\n",
      "135000/135000 [==============================] - 85s 626us/step - loss: 0.3167 - acc: 0.8664 - val_loss: 0.3797 - val_acc: 0.8345\n",
      "Epoch 6/10\n",
      "135000/135000 [==============================] - 84s 624us/step - loss: 0.3060 - acc: 0.8722 - val_loss: 0.3768 - val_acc: 0.8357\n",
      "Epoch 7/10\n",
      "135000/135000 [==============================] - 85s 627us/step - loss: 0.2969 - acc: 0.8765 - val_loss: 0.3820 - val_acc: 0.8311\n",
      "Epoch 8/10\n",
      "135000/135000 [==============================] - 73s 542us/step - loss: 0.2885 - acc: 0.8810 - val_loss: 0.3825 - val_acc: 0.8307\n",
      "Epoch 9/10\n",
      "135000/135000 [==============================] - 70s 518us/step - loss: 0.2795 - acc: 0.8851 - val_loss: 0.3896 - val_acc: 0.8303\n",
      "Epoch 10/10\n",
      "135000/135000 [==============================] - 84s 624us/step - loss: 0.2712 - acc: 0.8892 - val_loss: 0.3856 - val_acc: 0.8295\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "\n",
    "lstm_model3 = Sequential()\n",
    "lstm_model3.add(Embedding(input_dim = VOCAB_SIZE, output_dim=EMB_SIZE*2))\n",
    "lstm_model3.add(CuDNNLSTM(32))\n",
    "lstm_model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model3.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "lstm_history3 = lstm_model3.fit(input_train, label_train,\n",
    "                                epochs=10,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(input_eval, label_eval),\n",
    "                                callbacks=callbacks_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EMB_SIZE * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM\n",
    "\n",
    "lstm_model4 = Sequential()\n",
    "lstm_model4.add(Embedding(input_dim = VOCAB_SIZE, output_dim=EMB_SIZE*8))\n",
    "lstm_model4.add(CuDNNLSTM(32))\n",
    "lstm_model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model4.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "lstm_history4 = lstm_model3.fit(input_train, label_train,\n",
    "                                epochs=10,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(input_eval, label_eval),\n",
    "                                callbacks=callbacks_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
